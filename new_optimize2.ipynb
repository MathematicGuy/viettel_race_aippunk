{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b7c54ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/f8/xh6w51d525g30x00m8swhsjh0000gn/T/pip-req-build-m_zb0wyw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/f8/xh6w51d525g30x00m8swhsjh0000gn/T/pip-req-build-m_zb0wyw\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from clip==1.0) (2.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from clip==1.0) (0.23.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torch->clip==1.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torch->clip==1.0) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/demo/lib/python3.11/site-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5609b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTS =====\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pymilvus import MilvusClient, DataType, Function, FunctionType, AnnSearchRequest, RRFRanker\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebac18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12a41da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== MODEL INITIALIZATION =====\n",
    "# Text embedding model\n",
    "text_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Multimodal embedding model (CLIP cho text-image joint embedding)\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "\n",
    "# Image captioning model (InternVL3.5-2B-Instruct)\n",
    "# caption_tokenizer = AutoTokenizer.from_pretrained(\"OpenGVLab/InternVL3_5-2B-Instruct\", trust_remote_code=True)\n",
    "# caption_model = AutoModel.from_pretrained(\"OpenGVLab/InternVL3_5-2B-Instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ead0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MARKDOWN PROCESSOR =====\n",
    "class MarkdownProcessor:\n",
    "    \"\"\"Xử lý file markdown, trích xuất text, bảng, và hình ảnh\"\"\"\n",
    "\n",
    "    def __init__(self, markdown_dir: str, image_base_dir: str):\n",
    "        self.markdown_dir = Path(markdown_dir)\n",
    "        self.image_base_dir = Path(image_base_dir)\n",
    "\n",
    "    def extract_content(self, md_file: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Trích xuất text, tables, và images từ markdown\"\"\"\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            md_content = f.read()\n",
    "\n",
    "        html = markdown.markdown(md_content, extensions=['tables', 'fenced_code'])\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        text_content = soup.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        tables = []\n",
    "        for table in soup.find_all('table'):\n",
    "            table_text = table.get_text(separator=' | ', strip=True)\n",
    "            tables.append(table_text)\n",
    "\n",
    "        image_paths = []\n",
    "        img_pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "        for match in re.finditer(img_pattern, md_content):\n",
    "            img_path = match.group(1).strip()\n",
    "            if img_path.startswith(\"images/\"):\n",
    "                full_path = self.markdown_dir / img_path\n",
    "            else:\n",
    "                full_path = self.image_base_dir / img_path\n",
    "\n",
    "            if full_path.exists():\n",
    "                image_paths.append(str(full_path))\n",
    "\n",
    "        return {\n",
    "            'text': text_content,\n",
    "            'tables': tables,\n",
    "            'images': image_paths,\n",
    "            'source': str(md_file)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalImageEncoder:\n",
    "    \"\"\"Encode images and text with CLIP embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, clip_model, clip_preprocess):\n",
    "        self.clip_model = clip_model\n",
    "        self.clip_preprocess = clip_preprocess\n",
    "        self.device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Move model to device\n",
    "        self.clip_model = self.clip_model.to(self.device)\n",
    "\n",
    "    def encode_image_multimodal(self, image_path: str) -> List[float]:\n",
    "        \"\"\"Encode image into CLIP embedding vector\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_input = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_features = self.clip_model.encode_image(image_input)\n",
    "                image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            return image_features.squeeze().cpu().numpy().tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image {image_path}: {e}\")\n",
    "            # Return zero vector of appropriate dimension\n",
    "            return [0.0] * 512  # CLIP's standard dimension\n",
    "\n",
    "    def encode_text_for_image_search(self, text: str) -> List[float]:\n",
    "        \"\"\"Encode text into CLIP embedding vector for text-to-image search\"\"\"\n",
    "        try:\n",
    "            text_tokens = clip.tokenize(text).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_features = self.clip_model.encode_text(text_tokens)\n",
    "                text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            return text_features.squeeze().cpu().numpy().tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding text '{text}': {e}\")\n",
    "            # Return zero vector of appropriate dimension\n",
    "            return [0.0] * 512  # CLIP's standard dimension\n",
    "image_encoder = MultimodalImageEncoder(clip_model, clip_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a3cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ENTITY PROCESSORS =====\n",
    "class TextEntityProcessor:\n",
    "    \"\"\"Xử lý entity text riêng biệt\"\"\"\n",
    "\n",
    "    def __init__(self, text_model, chunk_size=500, overlap=100):\n",
    "        self.text_model = text_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "\n",
    "    def create_text_entities(self, content: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tạo text entities\"\"\"\n",
    "        full_text = content['text']\n",
    "\n",
    "        # Thêm tables vào text nếu có\n",
    "        if content['tables']:\n",
    "            full_text += \"\\n\\n\" + \"\\n\\n\".join(content['tables'])\n",
    "\n",
    "        text_chunks = self.text_splitter.create_documents([full_text])\n",
    "\n",
    "        entities = []\n",
    "        for idx, chunk in enumerate(text_chunks):\n",
    "            # Text embedding\n",
    "            text_embedding = self.text_model.encode(chunk.page_content, convert_to_tensor=False).tolist()\n",
    "\n",
    "            entity = {\n",
    "                # 'id': f\"text_{idx}\",\n",
    "                'content': chunk.page_content,\n",
    "                'text_dense': text_embedding,  # Dense vector cho semantic search\n",
    "                'text_sparse': chunk.page_content,  # Text cho BM25 sparse search\n",
    "                'metadata': {\n",
    "                    'entity_type': 'text',\n",
    "                    'source': content['source'],\n",
    "                    'chunk_index': idx,\n",
    "                    'content_type': 'text_with_tables' if content['tables'] else 'text_only'\n",
    "                },\n",
    "                # Các trường image để trống\n",
    "                'image_path': '',\n",
    "                # 'image_caption': '',\n",
    "                'image_dense': [0.0] * 512  # Vector trống với đúng dimension\n",
    "            }\n",
    "            entities.append(entity)\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8beeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEntityProcessor:\n",
    "    \"\"\"Xử lý entity image riêng biệt\"\"\"\n",
    "\n",
    "    def __init__(self, image_encoder):\n",
    "        self.image_encoder = image_encoder\n",
    "    \n",
    "    def create_image_entities(self, content: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Tạo image entities\"\"\"\n",
    "        entities = []\n",
    "\n",
    "        for idx, img_path in enumerate(content['images']):\n",
    "            # Generate caption\n",
    "            # caption = self.image_encoder.generate_caption(img_path)\n",
    "\n",
    "            # Multimodal embedding\n",
    "            image_embedding = self.image_encoder.encode_image_multimodal(img_path)\n",
    "\n",
    "            # Text embedding của caption (cho text search)\n",
    "            # caption_embedding = text_model.encode(caption, convert_to_tensor=False).tolist()\n",
    "\n",
    "            entity = {\n",
    "                # 'id': f\"image_{idx}\",\n",
    "                'content': \"\",  # Caption làm content chính\n",
    "                'image_path': img_path,\n",
    "                # 'image_caption': caption,\n",
    "                'image_dense': image_embedding,  # Multimodal vector cho image search\n",
    "                'metadata': {\n",
    "                    'entity_type': 'image',\n",
    "                    'source': content['source'],\n",
    "                    'image_index': idx,\n",
    "                    'original_path': img_path\n",
    "                },\n",
    "                # Các trường text bỏ trống cho image\n",
    "                'text_dense': [0.0] * 768,  # Dense vector của caption\n",
    "                # 'text_sparse': caption  # Caption text cho BM25\n",
    "            }\n",
    "            entities.append(entity)\n",
    "\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b2f9673",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextEntityProcessor(text_model)\n",
    "image_processor = ImageEntityProcessor(image_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== MILVUS HYBRID STORE =====\n",
    "class MilvusHybridStore:\n",
    "    \"\"\"Milvus store tối ưu cho hybrid search\"\"\"\n",
    "\n",
    "    def __init__(self, uri, token, collection_name=\"create_hybrid_pipeline_2\"):\n",
    "        self.client = MilvusClient(uri=uri, token=token)\n",
    "        self.collection_name = collection_name\n",
    "        self.text_dense_dim = 768  # paraphrase-multilingual-mpnet-base-v2\n",
    "        self.image_dense_dim = 512  # clip\n",
    "\n",
    "    def create_hybrid_collection(self):\n",
    "        \"\"\"Tạo collection cho hybrid search\"\"\"\n",
    "        if self.client.has_collection(self.collection_name):\n",
    "            self.client.drop_collection(self.collection_name)\n",
    "\n",
    "        schema = self.client.create_schema(\n",
    "            auto_id=True,\n",
    "            enable_dynamic_fields=True\n",
    "        )\n",
    "\n",
    "        # Primary key\n",
    "        schema.add_field(\n",
    "            field_name=\"id\",\n",
    "            datatype=DataType.VARCHAR,\n",
    "            is_primary=True,\n",
    "            max_length=100\n",
    "        )\n",
    "\n",
    "        # Content field\n",
    "        schema.add_field(\n",
    "            field_name=\"content\",\n",
    "            datatype=DataType.VARCHAR,\n",
    "            max_length=8000,\n",
    "            enable_analyzer=True,\n",
    "        )\n",
    "\n",
    "        # Text fields\n",
    "        schema.add_field(\n",
    "            field_name=\"text_dense\",\n",
    "            datatype=DataType.FLOAT_VECTOR,\n",
    "            dim=self.text_dense_dim,\n",
    "        )\n",
    "\n",
    "        schema.add_field(\n",
    "            field_name=\"text_sparse\",\n",
    "            datatype=DataType.SPARSE_FLOAT_VECTOR,\n",
    "        )\n",
    "\n",
    "        # Image fields\n",
    "        schema.add_field(\n",
    "            field_name=\"image_dense\",\n",
    "            datatype=DataType.FLOAT_VECTOR,\n",
    "            dim=self.image_dense_dim,\n",
    "        )\n",
    "\n",
    "        schema.add_field(\n",
    "            field_name=\"image_path\",\n",
    "            datatype=DataType.VARCHAR,\n",
    "            max_length=500,\n",
    "        )\n",
    "\n",
    "        # schema.add_field(\n",
    "        #     field_name=\"image_caption\",\n",
    "        #     datatype=DataType.VARCHAR,\n",
    "        #     max_length=1000,\n",
    "        # )\n",
    "\n",
    "        # Metadata\n",
    "        schema.add_field(\n",
    "            field_name=\"metadata\",\n",
    "            datatype=DataType.JSON\n",
    "        )\n",
    "\n",
    "        # BM25 function cho text sparse\n",
    "        bm25_function = Function(\n",
    "            name=\"text_bm25_emb\",\n",
    "            function_type=FunctionType.BM25,\n",
    "            input_field_names=[\"content\"],\n",
    "            output_field_names=[\"text_sparse\"]\n",
    "        )\n",
    "        schema.add_function(bm25_function)\n",
    "\n",
    "        # Index parameters\n",
    "        index_params = self.client.prepare_index_params()\n",
    "\n",
    "        # Text dense index\n",
    "        index_params.add_index(\n",
    "            field_name=\"text_dense\",\n",
    "            index_type=\"HNSW\",\n",
    "            metric_type=\"COSINE\",\n",
    "            params={\"M\": 16, \"efConstruction\": 200}\n",
    "        )\n",
    "\n",
    "        # Text sparse index\n",
    "        index_params.add_index(\n",
    "            field_name=\"text_sparse\",\n",
    "            index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "            metric_type=\"BM25\"\n",
    "        )\n",
    "\n",
    "        # Image dense index\n",
    "        index_params.add_index(\n",
    "            field_name=\"image_dense\",\n",
    "            index_type=\"HNSW\",\n",
    "            metric_type=\"COSINE\",\n",
    "            params={\"M\": 16, \"efConstruction\": 200}\n",
    "        )\n",
    "\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            schema=schema,\n",
    "            index_params=index_params\n",
    "        )\n",
    "\n",
    "        print(f\"✅ Hybrid collection '{self.collection_name}' đã được tạo!\")\n",
    "\n",
    "    def insert_entities(self, entities: List[Dict[str, Any]]):\n",
    "        \"\"\"Insert entities vào collection\"\"\"\n",
    "        batch_size = 50\n",
    "        for i in range(0, len(entities), batch_size):\n",
    "            batch = entities[i:i + batch_size]\n",
    "\n",
    "            # Remove text_sparse from entities as it's generated by BM25 function\n",
    "            for entity in batch:\n",
    "                entity.pop('text_sparse', None)\n",
    "\n",
    "            self.client.insert(\n",
    "                collection_name=self.collection_name,\n",
    "                data=batch\n",
    "            )\n",
    "            print(f\"✅ Đã insert batch {i//batch_size + 1}/{(len(entities)-1)//batch_size + 1}\")\n",
    "\n",
    "        print(f\"✅ Đã insert {len(entities)} entities vào Milvus!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import AnnSearchRequest, RRFRanker\n",
    "\n",
    "class HybridSearchEngine:\n",
    "    \"\"\"Engine cho hybrid search với multiple vectors từ text input\"\"\"\n",
    "\n",
    "    def __init__(self, milvus_store, text_model, image_encoder):\n",
    "        self.store = milvus_store\n",
    "        self.text_model = text_model\n",
    "        self.image_encoder = image_encoder\n",
    "\n",
    "    def hybrid_search(self, query_text: str, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"Thực hiện hybrid search chỉ với text input\"\"\"\n",
    "\n",
    "        search_requests = []\n",
    "\n",
    "        if not query_text:\n",
    "            return []\n",
    "\n",
    "        # 1. Text semantic search với text embedding model\n",
    "        query_text_embedding = self.text_model.encode(query_text, convert_to_tensor=False).tolist()\n",
    "\n",
    "        text_search = AnnSearchRequest(\n",
    "            data=[query_text_embedding],\n",
    "            anns_field=\"text_dense\",\n",
    "            param={\"nprobe\": 10},\n",
    "            limit=limit\n",
    "        )\n",
    "        search_requests.append(text_search)\n",
    "\n",
    "        # 2. Full-text search (BM25) với sparse vectors\n",
    "        sparse_search = AnnSearchRequest(\n",
    "            data=[query_text],\n",
    "            anns_field=\"text_sparse\",\n",
    "            param={\"drop_ratio_search\": 0.2},\n",
    "            limit=limit\n",
    "        )\n",
    "        search_requests.append(sparse_search)\n",
    "\n",
    "        # 3. Multimodal search - embedding text bằng image encoder\n",
    "        query_image_embedding = self.image_encoder.encode_text_for_image_search(query_text)\n",
    "\n",
    "        image_search = AnnSearchRequest(\n",
    "            data=[query_image_embedding],\n",
    "            anns_field=\"image_dense\",\n",
    "            param={\"nprobe\": 10},\n",
    "            limit=limit\n",
    "        )\n",
    "        search_requests.append(image_search)\n",
    "\n",
    "        # Thực hiện hybrid search với RRF ranker\n",
    "        ranker = RRFRanker(100)\n",
    "\n",
    "        results = self.store.client.hybrid_search(\n",
    "            collection_name=self.store.collection_name,\n",
    "            reqs=search_requests,\n",
    "            ranker=ranker,\n",
    "            limit=limit,\n",
    "            output_fields=[\"content\", \"metadata\", \"image_path\"]\n",
    "        )\n",
    "\n",
    "        return results[0] if results else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01baab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MAIN PIPELINE =====\n",
    "def create_hybrid_pipeline(markdown_dir: str, image_base_dir: str,\n",
    "                          uri: str, token: str):\n",
    "    \"\"\"Tạo pipeline hybrid search hoàn chỉnh\"\"\"\n",
    "\n",
    "    # Initialize components\n",
    "    processor = MarkdownProcessor(markdown_dir, image_base_dir)\n",
    "    store = MilvusHybridStore(uri, token)\n",
    "    search_engine = HybridSearchEngine(store, text_model, image_encoder)\n",
    "\n",
    "    # Tạo collection\n",
    "    store.create_hybrid_collection()\n",
    "\n",
    "    def process_documents(markdown_files: List[str]):\n",
    "        \"\"\"Xử lý documents thành entities\"\"\"\n",
    "        all_entities = []\n",
    "\n",
    "        for md_file in markdown_files:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Đang xử lý: {md_file}\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "            # Extract content\n",
    "            content = processor.extract_content(Path(md_file))\n",
    "            print(f\"  - Text length: {len(content['text'])} ký tự\")\n",
    "            print(f\"  - Số bảng: {len(content['tables'])}\")\n",
    "            print(f\"  - Số hình ảnh: {len(content['images'])}\")\n",
    "\n",
    "            # Tạo text entities\n",
    "            text_entities = text_processor.create_text_entities(content)\n",
    "            print(f\"  - Text entities: {len(text_entities)}\")\n",
    "\n",
    "            # Tạo image entities\n",
    "            image_entities = image_processor.create_image_entities(content)\n",
    "            print(f\"  - Image entities: {len(image_entities)}\")\n",
    "\n",
    "            all_entities.extend(text_entities)\n",
    "            all_entities.extend(image_entities)\n",
    "\n",
    "                # Insert vào Milvus\n",
    "        store.insert_entities(all_entities)\n",
    "\n",
    "        print(f\"\\n✅ Pipeline hoàn tất! Tổng cộng {len(all_entities)} entities đã được xử lý.\")\n",
    "        return all_entities, search_engine\n",
    "\n",
    "    return process_documents, search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30d7b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MARKDOWN_DIR = 'test_data'\n",
    "IMAGE_BASE_DIR = 'test_data/images'\n",
    "MILVUS_URI = \"https://in03-820213f77a18314.serverless.aws-eu-central-1.cloud.zilliz.com\"\n",
    "MILVUS_TOKEN = \"3e35fd5ea0bdf741db327f1e34058b16a3f4634904e7fc717ca74238836e0fb9ef0aee7727d97a9276e46281ec07399bdc03f38e\"\n",
    "# MILVUS_URI = \"điền uri vào đây\"\n",
    "# MILVUS_TOKEN = \"điền token vào đây\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afdd152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hybrid collection 'hybrid_rag_collection' đã được tạo!\n"
     ]
    }
   ],
   "source": [
    "# Tạo pipeline\n",
    "process_docs, search_engine = create_hybrid_pipeline(\n",
    "     MARKDOWN_DIR, IMAGE_BASE_DIR, MILVUS_URI, MILVUS_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9184404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Đang xử lý: test_data/Public048.md\n",
      "============================================================\n",
      "  - Text length: 5734 ký tự\n",
      "  - Số bảng: 4\n",
      "  - Số hình ảnh: 11\n",
      "  - Text entities: 16\n",
      "  - Image entities: 11\n",
      "\n",
      "============================================================\n",
      "Đang xử lý: test_data/Public043.md\n",
      "============================================================\n",
      "  - Text length: 9891 ký tự\n",
      "  - Số bảng: 7\n",
      "  - Số hình ảnh: 14\n",
      "  - Text entities: 28\n",
      "  - Image entities: 14\n",
      "✅ Đã insert batch 1/2\n",
      "✅ Đã insert batch 2/2\n",
      "✅ Đã insert 69 entities vào Milvus!\n",
      "\n",
      "✅ Pipeline hoàn tất! Tổng cộng 69 entities đã được xử lý.\n"
     ]
    }
   ],
   "source": [
    "# Xử lý documents\n",
    "markdown_files = list(Path(MARKDOWN_DIR).glob(\"*.md\"))\n",
    "entities, search_engine = process_docs(markdown_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "883556ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e079569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"TEST HYBRID SEARCH\")\n",
    "# print(f\"{'='*60}\")\n",
    "\n",
    "# Text search\n",
    "text_results = search_engine.hybrid_search(\n",
    "    query_text=\"kernel là gì ?\",\n",
    "    limit=5\n",
    ")\n",
    "# print(f\"Text search results: {len(text_results)}\")\n",
    "# for i, result in enumerate(text_results):\n",
    "#     pprint(f\"  {i+1}. Score: {result['distance']:.4f}\")\n",
    "#     pprint(f\"     Content: {result['entity']['content']}...\")\n",
    "#     pprint(f\"     Type: {result['entity']['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfefe7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '461027867908063963', 'distance': 0.01951637491583824, 'entity': {'content': '2. Phép tính convolution\\n2.1 Convolution\\nĐể cho dễ hình dung tôi sẽ lấy ví dụ trên ảnh xám, tức là ảnh được biểu diễn dưới dạng ma trận A kích thước $\\\\mathfrak { m } ^ { * } \\\\mathfrak { n }$ .\\nTa định nghĩa kernel là một ma trận vuông kích thước $\\\\mathrm { k ^ { * } k }$ trong đó k là số lẻ. k có thể bằng 1, 3, 5, 7, 9,... Ví dụ kernel kích thước $3 ^ { * } 3$\\n$$\\nW = { \\\\left[ \\\\begin{array} { l l l } { 1 } & { 0 } & { 1 } \\\\ { 0 } & { 1 } & { 0 } \\\\ { 1 } & { 0 } & { 1 } \\\\end{array} \\\\right] }\\n$$', 'image_path': '', 'metadata': {'entity_type': 'text', 'source': 'test_data/Public043.md', 'chunk_index': 16, 'content_type': 'text_with_tables'}}}, {'id': '461027867908063950', 'distance': 0.01913919486105442, 'entity': {'content': 'Bạn sẽ thấy chiều dài ảnh là 800 pixels (viết tắt px), chiều rộng 600 pixels, kích thước là $8 0 0 ^ { * } 6 0 0$ . Trước giờ chỉ học đơn vị đo là mét hay centimet, pixel là gì nhỉ ?\\nTheo wiki, pixel (hay điểm ảnh) là một khối màu rất nhỏ và là đơn vị cơ bản nhất để tạo nên một bức ảnh kỹ thuật số.\\nVậy bức ảnh trên kích thước 800 pixel $* 6 0 0$ pixel, có thể biểu diễn dưới dạng một ma trận kích thước $6 0 0 ^ { * } 8 0 0$ (vì định nghĩa ma trận là số hàng nhân số cột).', 'image_path': '', 'metadata': {'entity_type': 'text', 'source': 'test_data/Public043.md', 'chunk_index': 3, 'content_type': 'text_with_tables'}}}, {'id': '461027867908063944', 'distance': 0.009900989942252636, 'entity': {'content': '', 'image_path': 'test_data/images/5fab136d11d3809e8ab8a99ca0cac6d6906802498095bf2f906fae9358154f4c.jpg', 'metadata': {'entity_type': 'image', 'source': 'test_data/Public048.md', 'image_index': 8, 'original_path': 'test_data/images/5fab136d11d3809e8ab8a99ca0cac6d6906802498095bf2f906fae9358154f4c.jpg'}}}, {'id': '461027867908063951', 'distance': 0.009900989942252636, 'entity': {'content': 'Hình 7.3: Mathematical bridge, Cambridge\\nTrong đó mỗi phần tử $w _ { i j }$ là một pixel.\\nNhư vậy có thể hiểu là mỗi pixel thì biểu diễn một màu và bức ảnh trên là sự kết hợp rất nhiều pixel. Hiểu đơn giản thì in bức ảnh ra, kẻ ô vuông như chơi cờ ca rô với 800\\nđường thẳng ở chiều dài, 600 đường ở chiều rộng, thì mỗi ô vuông là một pixel, biểu diễn một chấm màu.', 'image_path': '', 'metadata': {'entity_type': 'text', 'source': 'test_data/Public043.md', 'chunk_index': 4, 'content_type': 'text_with_tables'}}}, {'id': '461027867908063941', 'distance': 0.009803921915590763, 'entity': {'content': '', 'image_path': 'test_data/images/17ff71a127b22467e6fcd63a32df4dc372e69e0da82d0f9b7a2fd195ed557754.jpg', 'metadata': {'entity_type': 'image', 'source': 'test_data/Public048.md', 'image_index': 5, 'original_path': 'test_data/images/17ff71a127b22467e6fcd63a32df4dc372e69e0da82d0f9b7a2fd195ed557754.jpg'}}}]\n"
     ]
    }
   ],
   "source": [
    "pprint(text_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140825e",
   "metadata": {},
   "source": [
    "### Load câu hỏi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efca097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/demo/lib/python3.11/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9582cadee264accb19e224011a1558b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Giả sử search_engine.hybrid_search() đã được định nghĩa\n",
    "# Nếu không, bạn cần cung cấp chi tiết để tích hợp\n",
    "\n",
    "def load_questions_from_csv(csv_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Đọc câu hỏi và đáp án từ file CSV.\n",
    "    CSV có các cột: Question, A, B, C, D\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    questions = []\n",
    "    for _, row in df.iterrows():\n",
    "        question_data = {\n",
    "            'question': row['Question'],\n",
    "            'options': {\n",
    "                'A': row['A'],\n",
    "                'B': row['B'],\n",
    "                'C': row['C'],\n",
    "                'D': row['D']\n",
    "            }\n",
    "        }\n",
    "        questions.append(question_data)\n",
    "    return questions\n",
    "\n",
    "def perform_vector_search(search_engine, query: str, k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Thực hiện vector search cho một query, trả về top k kết quả.\n",
    "    \"\"\"\n",
    "    return search_engine.hybrid_search(query_text=query, limit=k)\n",
    "\n",
    "def collect_contexts(search_engine, question: str, options: Dict[str, str], k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Thực hiện vector search cho câu hỏi và từng đáp án, gộp context.\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "\n",
    "    # Search cho câu hỏi\n",
    "    question_results = perform_vector_search(search_engine, question, k)\n",
    "    contexts.extend(question_results)\n",
    "\n",
    "    # Search cho từng đáp án\n",
    "    for option_key, option_text in options.items():\n",
    "        option_results = perform_vector_search(search_engine, option_text, k)\n",
    "        contexts.extend(option_results)\n",
    "\n",
    "    return contexts\n",
    "\n",
    "def format_contexts(contexts: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Định dạng context (text và ảnh) thành chuỗi để đưa vào prompt.\n",
    "    \"\"\"\n",
    "    formatted_context = \"\"\n",
    "    for idx, ctx in enumerate(contexts, 1):\n",
    "        content = ctx['entity'].get('content', '')\n",
    "        image_path = ctx['entity'].get('image_path', '')\n",
    "        distance = ctx['distance']\n",
    "        source = ctx['entity']['metadata'].get('source', 'unknown')\n",
    "\n",
    "        formatted_context += f\"Context {idx} (Source: {source}, Distance: {distance:.4f}):\\n\"\n",
    "        if content:\n",
    "            formatted_context += f\"Text: {content}\\n\"\n",
    "        if image_path:\n",
    "            formatted_context += f\"Image: {image_path}\\n\"\n",
    "        formatted_context += \"\\n\"\n",
    "\n",
    "    return formatted_context\n",
    "\n",
    "def create_prompt(question: str, options: Dict[str, str], contexts: str) -> str:\n",
    "    \"\"\"\n",
    "    Tạo prompt cho LLM dựa trên câu hỏi, đáp án và context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Bạn là một trợ lý AI thông minh, được cung cấp context từ vector search để trả lời câu hỏi trắc nghiệm. Dựa trên context, hãy chọn đáp án đúng (có thể có nhiều đáp án đúng) từ các lựa chọn A, B, C, D. Trả về kết quả dưới dạng: \"Đáp án đúng: [A, B, C, D]\".\n",
    "\n",
    "**Câu hỏi**: {question}\n",
    "\n",
    "**Các đáp án**:\n",
    "A: {options['A']}\n",
    "B: {options['B']}\n",
    "C: {options['C']}\n",
    "D: {options['D']}\n",
    "\n",
    "**Context**:\n",
    "{contexts}\n",
    "\n",
    "**Yêu cầu**:\n",
    "- Phân tích context (bao gồm text và ảnh nếu có) để xác định đáp án đúng.\n",
    "- Nếu context chứa ảnh, hãy mô tả cách ảnh liên quan đến câu hỏi (nếu có thể).\n",
    "- Trả về đáp án đúng dưới dạng: \"Đáp án đúng: [A, B, C, D]\".\n",
    "- Nếu không đủ thông tin, hãy trả về: \"Không đủ thông tin để xác định đáp án đúng.\"\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def initialize_model(model_name: str = \"Qwen/Qwen2.5-VL-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Khởi tạo model Qwen2.5-VL-3B-Instruct và processor.\n",
    "    \"\"\"\n",
    "    # Load processor (handles both text and image tokenization)\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForVision2Seq.from_pretrained(model_name)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "def generate_response(model, processor, prompt: str, image_paths: List[str] = None, max_new_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    Gọi model Qwen2.5-VL-3B-Instruct để tạo phản hồi từ prompt và ảnh (nếu có).\n",
    "    \"\"\"\n",
    "    # Chuẩn bị conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Thêm ảnh vào conversation nếu có\n",
    "    if image_paths:\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                conversation[0][\"content\"].append({\"type\": \"image\", \"image\": image})\n",
    "            except Exception as e:\n",
    "                print(f\"Không thể load ảnh {img_path}: {e}\")\n",
    "\n",
    "    # Áp dụng template và tokenize\n",
    "    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(\n",
    "        text=[text_prompt],\n",
    "        images=[content[\"image\"] for content in conversation[0][\"content\"] if content[\"type\"] == \"image\"] or None,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Tạo phản hồi từ model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,  # Không sử dụng sampling để đảm bảo kết quả chính xác\n",
    "        num_beams=5,      # Beam search để cải thiện chất lượng\n",
    "    )\n",
    "\n",
    "    # Decode phản hồi\n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def rag_pipeline(csv_path: str, search_engine):\n",
    "    \"\"\"\n",
    "    Pipeline RAG hoàn chỉnh: đọc câu hỏi, vector search, gọi model, trả kết quả.\n",
    "    \"\"\"\n",
    "    # Khởi tạo model và processor\n",
    "    model, processor = initialize_model(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "    # Đọc câu hỏi từ CSV\n",
    "    questions = load_questions_from_csv(csv_path)\n",
    "    results = []\n",
    "\n",
    "    for q_data in questions:\n",
    "        question = q_data['question']\n",
    "        options = q_data['options']\n",
    "\n",
    "        # Thu thập context từ vector search\n",
    "        contexts = collect_contexts(search_engine, question, options)\n",
    "\n",
    "        # Lấy danh sách đường dẫn ảnh từ context\n",
    "        image_paths = [ctx['entity'].get('image_path', '') for ctx in contexts if ctx['entity'].get('image_path')]\n",
    "        image_paths = [path for path in image_paths if path]  # Loại bỏ các path rỗng\n",
    "\n",
    "        # Định dạng context\n",
    "        formatted_contexts = format_contexts(contexts)\n",
    "\n",
    "        # Tạo prompt\n",
    "        prompt = create_prompt(question, options, formatted_contexts)\n",
    "\n",
    "        # Gọi model để suy luận\n",
    "        response = generate_response(model, processor, prompt, image_paths)\n",
    "\n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'options': options,\n",
    "            'response': response\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Ví dụ sử dụng\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"training_input/question.csv\"\n",
    "    # search_engine: đối tượng search engine của bạn\n",
    "    # Ví dụ: search_engine = YourSearchEngineClass()\n",
    "\n",
    "    results = rag_pipeline(csv_path, search_engine)\n",
    "\n",
    "    # In kết quả\n",
    "    for result in results:\n",
    "        print(f\"Câu hỏi: {result['question']}\")\n",
    "        print(f\"Đáp án: {result['options']}\")\n",
    "        print(f\"Kết quả: {result['response']}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
