{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "120c1a6f",
   "metadata": {},
   "source": [
    "### Offline Model download Example\n",
    "`!docling-tools models download-hf-repo ds4sd/SmolDocling-256M-preview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52d2485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import docling\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling_core.types.io import DocumentStream\n",
    "from docling.datamodel.pipeline_options import TableFormerMode\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.document import ConversionResult\n",
    "from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    VlmPipelineOptions,\n",
    ")\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9fcc0c",
   "metadata": {},
   "source": [
    "### Hugging Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda5f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_ocr_model():\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_use_double_quant=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     )\n",
    "\n",
    "\n",
    "#     model = AutoModelForImageTextToText.from_pretrained(\n",
    "#         MODEL_PATH,\n",
    "#         # quantization_config=bnb_config,\n",
    "#         dtype=torch.bfloat16,\n",
    "#         device_map=\"auto\",\n",
    "#         attn_implementation=\"flash_attention_2\",\n",
    "#         # llm_int8_enable_fp32_cpu_offload=True,\n",
    "#     )\n",
    "\n",
    "#     torch.backends.cuda.matmul.allow_tf32 = True\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     torch.set_float32_matmul_precision(\"high\")\n",
    "#     model.eval()\n",
    "\n",
    "#     processor = AutoProcessor.from_pretrained(MODEL_PATH)\n",
    "#     return model, processor\n",
    "\n",
    "\n",
    "# def run_nanonet_ocr(image_path: str, model, processor) -> str:\n",
    "#     \"\"\"Run local OCR inference with Nanonet-OCR2-3B.\"\"\"\n",
    "#     image = Image.open(image_path).convert(\"RGB\")\n",
    "#     inputs = processor(images=image, text=\"\", return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.inference_mode():\n",
    "#         outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "#     text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "#     return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84f48a",
   "metadata": {},
   "source": [
    "### Docling Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "import docling\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "\n",
    "\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling_core.types.io import DocumentStream\n",
    "from docling.datamodel.pipeline_options import TableFormerMode\n",
    "from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.document import ConversionResult\n",
    "from docling.datamodel.pipeline_options_vlm_model import ApiVlmOptions, ResponseFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    VlmPipelineOptions,\n",
    ")\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "import time\n",
    "import socket\n",
    "\n",
    "# Step 1 — Set env vars before starting server\n",
    "os.environ[\"OLLAMA_FLASH_ATTENTION\"] = \"true\"\n",
    "os.environ[\"OLLAMA_LOW_VRAM\"] = \"false\"\n",
    "os.environ[\"OLLAMA_NUM_PARALLEL\"] = \"4\"\n",
    "os.environ[\"OLLAMA_CONTEXT_LENGTH\"] = \"2048\"\n",
    "\n",
    "# Step 2 — Start Ollama server with these variables\n",
    "\n",
    "def is_ollama_running():\n",
    "\tsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\tresult = sock.connect_ex(('localhost', 11434))\n",
    "\tsock.close()\n",
    "\treturn result == 0\n",
    "\n",
    "if not is_ollama_running():\n",
    "\tollama_proc = subprocess.Popen([\"ollama\", \"serve\"], env=os.environ)\n",
    "\ttime.sleep(5)  # wait for Ollama to be ready\n",
    "else:\n",
    "\tprint(\"Ollama is already running\")\n",
    "\n",
    "\n",
    "# MODEL_PATH = \"benhaotang/Nanonets-OCR-s:latest\"  # local HF model\n",
    "MODEL_PATH = \"benhaotang/Nanonets-OCR-s:q4_k_m\"\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'info'\n",
    "\n",
    "\n",
    "def create_vlm_options(model:str, prompt:str):\n",
    "    options = ApiVlmOptions(\n",
    "        url=\"http://localhost:11434/v1/chat/completions\",  # the default Ollama endpoint # type: ignore\n",
    "        params=dict(\n",
    "            model=model,\n",
    "        ),\n",
    "        prompt=prompt,\n",
    "        timeout=350,\n",
    "        scale=1.0,\n",
    "        response_format=ResponseFormat.MARKDOWN,\n",
    "    )\n",
    "\n",
    "    return options\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 2️⃣ Configure Docling\n",
    "# ===========================\n",
    "def doc_converter():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    system_prompt = \"\"\"\n",
    "        Hãy trích xuất toàn bộ văn bản từ tài liệu ở trên giống như cách bạn đọc nó một cách tự nhiên.\n",
    "        Trả về các bảng dưới dạng mã HTML.\n",
    "        Trả về các phương trình dưới dạng biểu diễn LaTeX.\n",
    "\n",
    "        Nếu trong tài liệu có hình ảnh nhưng không có chú thích, hãy thêm một mô tả ngắn cho hình ảnh đó bên trong thẻ <img></img>;\n",
    "        nếu hình ảnh đã có chú thích, hãy đặt chú thích đó bên trong thẻ <img></img>.\n",
    "\n",
    "        Dấu watermark nên được đặt trong thẻ <watermark></watermark>.\n",
    "        Số trang nên được đặt trong thẻ <page_number></page_number>.\n",
    "        Ví dụ: <page_number>14</page_number> hoặc <page_number>9/22</page_number>.\n",
    "\n",
    "        Ưu tiên sử dụng ký hiệu ☐ và ☑ cho các ô kiểm (checkbox).\n",
    "\n",
    "        QUAN TRỌNG: Luôn luôn trả lời bằng **Tiếng Việt**.\n",
    "    \"\"\"\n",
    "\n",
    "    #? run remote Ollama, Huggingface model\n",
    "    pdf_options = VlmPipelineOptions(\n",
    "        enable_remote_services=True  # required when calling remote VLM endpoints\n",
    "    )\n",
    "    pdf_options.accelerator_options = AcceleratorOptions(\n",
    "        num_threads=12, device=AcceleratorDevice.AUTO\n",
    "    )\n",
    "    pdf_options.vlm_options = create_vlm_options(\n",
    "        model=MODEL_PATH,\n",
    "        prompt=system_prompt,\n",
    "    )\n",
    "\n",
    "    converter = DocumentConverter(\n",
    "        # allowed_formats=[\n",
    "        #     InputFormat.PDF,\n",
    "        #     InputFormat.IMAGE,\n",
    "        #     InputFormat.DOCX,\n",
    "        #     InputFormat.HTML,\n",
    "        #     InputFormat.PPTX,\n",
    "        # ],  # whitelist formats, non-matching files are igno\n",
    "\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_options=pdf_options,\n",
    "                pipeline_cls=VlmPipeline,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return converter\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 3️⃣ Main PDF → Markdown pipeline\n",
    "# ===========================\n",
    "def extract_text_to_markdown(input_dir: str, output_dir: str):\n",
    "    output_dir = Path('scratch') # type: ignore\n",
    "    output_dir.mkdir(parents=True, exist_ok=True) # type: ignore\n",
    "\n",
    "    data_folder = Path(__file__).parent / input_dir # get absoluate path, inefficient but get the job done.\n",
    "    file_names = os.listdir(data_folder)\n",
    "    file_list = [data_folder/file_name for file_name in file_names]\n",
    "\n",
    "\n",
    "    # process_mul_files_example = [\n",
    "    #     \"tests/data/html/wiki_duck.html\",\n",
    "    #     \"tests/data/docx/word_sample.docx\",\n",
    "    #     \"tests/data/docx/lorem_ipsum.docx\",\n",
    "    #     \"tests/data/pptx/powerpoint_sample.pptx\",\n",
    "    #     \"tests/data/2305.03393v1-pg9-img.png\",\n",
    "    #     \"tests/data/pdf/2206.01062.pdf\",\n",
    "    # ]\n",
    "\n",
    "    converter = doc_converter() # DocumentConverter()\n",
    "    conv_results_iter = converter.convert_all(file_list) # extract multiple files\n",
    "    docs = [result.document for result in conv_results_iter]\n",
    "\n",
    "    # with open(file_path, \"rb\") as f:\n",
    "    #     body_stream = BytesIO(f.read())\n",
    "\n",
    "    for file_name, doc in zip(file_names, docs):\n",
    "        with open(f\"{output_dir}/{file_name[:-4]}.md\", \"w\", encoding='utf-8') as f:\n",
    "            f.write(doc.export_to_markdown())\n",
    "\n",
    "    # ollama_proc.terminate() # AUTO turn off Ollama\n",
    "    # return markdown_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3948f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.perf_counter()  # Use perf_counter for higher precision\n",
    "    extract_text_to_markdown('private-test-input', 'scratch')\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time:.4f} seconds\")\n",
    "    ollama_proc.terminate() # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
