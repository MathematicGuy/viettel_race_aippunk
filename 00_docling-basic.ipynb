{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5076f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4632905",
   "metadata": {},
   "source": [
    "# Document folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdb61f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WindowsPath('private-test-input/Public001.pdf'), WindowsPath('private-test-input/Public002.pdf'), WindowsPath('private-test-input/Public003.pdf'), WindowsPath('private-test-input/Public004.pdf'), WindowsPath('private-test-input/Public005.pdf'), WindowsPath('private-test-input/question.csv')]\n"
     ]
    }
   ],
   "source": [
    "doc_dir = \"private-test-input\"\n",
    "\n",
    "files = list(Path(doc_dir).glob(\"*\"))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe772cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "private-test-input\\Public001.pdf\n"
     ]
    }
   ],
   "source": [
    "file = files[0]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2fd1dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55176b0",
   "metadata": {},
   "source": [
    "# Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95c5812",
   "metadata": {},
   "source": [
    "Using `DocumentConverter` to process a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2e5e80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e54892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 23:16:05,486 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-15 23:16:05,521 - INFO - Going to convert document batch...\n",
      "2025-10-15 23:16:05,523 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 4f2edc0f7d9bb60b38ebfecf9a2609f5\n",
      "2025-10-15 23:16:05,532 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-15 23:16:05,532 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-15 23:16:05,543 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-15 23:16:05,549 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-15 23:16:05,550 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-10-15 23:16:05,551 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-10-15 23:16:05,856 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:05,874 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:05,879 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:08,575 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:09,315 [RapidOCR] download_file.py:95: Successfully saved to: D:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:09,316 [RapidOCR] torch.py:54: Using D:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:09,480 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:09,484 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:11,638 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:11,681 [RapidOCR] download_file.py:95: Successfully saved to: D:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:11,684 [RapidOCR] torch.py:54: Using D:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:11,750 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:11,752 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:14,131 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:15,474 [RapidOCR] download_file.py:95: Successfully saved to: D:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-10-15 23:16:15,474 [RapidOCR] torch.py:54: Using D:\\Utilities\\miniconda\\envs\\t\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-10-15 23:16:15,662 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-10-15 23:16:15,671 - INFO - Accelerator device: 'cuda:0'\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-10-15 23:16:20,525 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-10-15 23:16:20,540 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-10-15 23:16:39,593 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-10-15 23:16:40,126 - INFO - Processing document Public001.pdf\n",
      "2025-10-15 23:16:46,354 - INFO - Finished converting document Public001.pdf in 40.88 sec.\n"
     ]
    }
   ],
   "source": [
    "source = file\n",
    "converter = DocumentConverter()\n",
    "doc = converter.convert(source).document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd662c28",
   "metadata": {},
   "source": [
    "Serialize the converted result with `export_to_...` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8ae1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## VIETTEL AI RACE\n",
      "\n",
      "## NGHIÊN C Ứ U IOT Ứ NG D Ụ NG XÂY D Ự NG MÔ HÌNH NHÀ THÔNG MINH\n",
      "\n",
      "TD001\n",
      "\n",
      "L ầ n ban hành: 1\n",
      "\n",
      "Công ngh ệ ứ ng d ụ ng vào mô hình nhà thông minh d ự a trên Internet v ạ n v ậ t (IoT - Internet of Things) ph ầ n l ớ n b ị h ạ n ch ế và phân tán. Các đánh giá trong bài viết đượ c th ự c hi ện để phân lo ạ i b ố i c ả nh nghiên c ứ u v ề ứ ng d ụ ng IoT xây d ự ng mô hình nhà thông minh, nh ằ m cung c ấ p nh ữ ng hi ể u bi ế t có giá tr ị v ề công n\n"
     ]
    }
   ],
   "source": [
    "print(doc.export_to_markdown()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329a362",
   "metadata": {},
   "source": [
    "# Annotate images with OpenAI API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c9d3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    PdfPipelineOptions,\n",
    "    PictureDescriptionApiOptions\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling_core.types.doc.document import ImageRefMode\n",
    "\n",
    "def convert_with_image_annotation(input_doc_path):\n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    model = \"gpt-4.1-mini\"\n",
    "    picture_desc_api_option = PictureDescriptionApiOptions(\n",
    "        url=\"https://api.openai.com/v1/chat/completions\",\n",
    "        prompt=\"Describe this image in sentences in a single paragraph.\",\n",
    "        params=dict(\n",
    "                model=model,\n",
    "            ),\n",
    "        headers={\n",
    "            \"Authorization\": \"Bearer \" + api_key,\n",
    "        },\n",
    "        timeout=60,\n",
    "    )\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions(\n",
    "        do_picture_description=True,\n",
    "        picture_description_options=picture_desc_api_option,\n",
    "        enable_remote_services=True,\n",
    "        generate_picture_images=True,\n",
    "        images_scale=2,\n",
    "    )\n",
    "\n",
    "    converter = DocumentConverter(\n",
    "        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n",
    "    )\n",
    "    conv_res = converter.convert(source=input_doc_path)\n",
    "    return conv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d3a9817",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"NoneType\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mconvert_with_image_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mconvert_with_image_annotation\u001b[39m\u001b[34m(input_doc_path)\u001b[39m\n\u001b[32m     10\u001b[39m api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m model = \u001b[33m\"\u001b[39m\u001b[33mgpt-4.1-mini\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m picture_desc_api_option = PictureDescriptionApiOptions(\n\u001b[32m     13\u001b[39m     url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     prompt=\u001b[33m\"\u001b[39m\u001b[33mDescribe this image in sentences in a single paragraph.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     params=\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     16\u001b[39m             model=model,\n\u001b[32m     17\u001b[39m         ),\n\u001b[32m     18\u001b[39m     headers={\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33;43m\"\u001b[39;49m\u001b[33;43mBearer \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m,\n\u001b[32m     20\u001b[39m     },\n\u001b[32m     21\u001b[39m     timeout=\u001b[32m60\u001b[39m,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m pipeline_options = PdfPipelineOptions(\n\u001b[32m     25\u001b[39m     do_picture_description=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     26\u001b[39m     picture_description_options=picture_desc_api_option,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     images_scale=\u001b[32m2\u001b[39m,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m converter = DocumentConverter(\n\u001b[32m     33\u001b[39m     format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)}\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: can only concatenate str (not \"NoneType\") to str"
     ]
    }
   ],
   "source": [
    "result = convert_with_image_annotation(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337829fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- Extracts metadata from the document, such as title, authors, references and language\n",
      "- Optionally applies OCR, e.g. for scanned PDFs\n",
      "- Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "```\n",
      "from docling.document_converter import DocumentConverter Large\n",
      "```\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!--<annotation kind=\"description\">-->The image illustrates a PDF processing workflow starting with a PDF file, which is parsed into individual pages. These pages then enter a model pipeline consisting of three main stages: OCR (Optical Character Recognition) to extract textual content, Layout Analysis to identify the spatial arrangement of elements like text blocks and images, and Table Structure analysis to detect and organize tabular data. After passing through the pipeline, the results are assembled and undergo document post-processing. Finally, the processed data is serialized into a structured format such as JSON or Markdown for further use or analysis.<!--<annotation/>-->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n"
     ]
    }
   ],
   "source": [
    "print(result.document.export_to_markdown(mark_annotations = True, include_annotations=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befad21c",
   "metadata": {},
   "source": [
    "# Save to MD with external referenced images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_function_md_with_image_ref(conv_res, output_path:str, replace_blank:str=\"_\"):\n",
    "\n",
    "    output_dir = Path(output_path)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem.replace(\" \", replace_blank)\n",
    "\n",
    "    # Save markdown with externally referenced pictures\n",
    "    md_filename = output_dir / f\"{doc_filename}-with-image-refs.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED, include_annotations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e300c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_function_md_with_image_ref(result, \"outputs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
